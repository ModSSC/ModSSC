run:
  name: best_text_inductive_mixmatch_imdb_p20
  seed: 2
  output_dir: runs/p20/inductive/mixmatch/text/imdb
  log_level: detailed
  fail_fast: true

limits:
  profile: auto

dataset:
  id: imdb
  download: true
  options:
    text_column: text
    label_column: label
    prefer_test_split: true
sampling:
  seed: 2
  plan:
    split:
      kind: holdout
      test_fraction: 0.2
      val_fraction: 0.1
      stratify: true
      shuffle: true
    labeling:
      mode: fraction
      value: 0.20
      strategy: balanced
      min_per_class: 1
    imbalance:
      kind: none
    policy:
      respect_official_test: true
      use_official_graph_masks: true
      allow_override_official: false
preprocess:
  seed: 2
  fit_on: train_labeled
  cache: true
  plan:
    output_key: features.X
    steps:
    - id: labels.encode
    - id: labels.to_torch
      params:
        device: "auto"
        dtype: int64
    - id: text.ensure_strings
    - id: text.vocab_tokenizer
      params:
        vocab_size: 20000
        max_length: 256
    - id: core.to_torch
      params:
        device: "auto"
        dtype: int64
augmentation:
  enabled: true
  seed: 2
  mode: fixed
  modality: text
  # We should use text augmentations here if we had them.
  # For now, disable tabular augmentations or implement simple ones.
  weak:
    steps: []
  strong:
    steps: []
method:
  kind: inductive
  id: mixmatch
  device:
    device: "auto"
    dtype: float32
  params:
    lambda_u: 1.0
    temperature: 0.5
    mixup_alpha: 0.5
    unsup_warm_up: 0.4
    mixup_manifold: false
    freeze_bn: false
    batch_size: 128
    max_epochs: 50
  model:
    classifier_id: lstm_scratch
    classifier_backend: torch
    classifier_params:
      vocab_size: 20000
      embedding_dim: 128
      hidden_dim: 128
      num_layers: 1
      lr: 0.001
      weight_decay: 0.0
      batch_size: 128
      max_epochs: 50
    ema: false
evaluation:
  split_for_model_selection: val
  report_splits:
  - val
  - test
  metrics:
  - accuracy
  - macro_f1
